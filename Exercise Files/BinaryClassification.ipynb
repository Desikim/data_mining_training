{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARY CLASSIFICATION OF SENTIMENT\n",
    "\n",
    "**File:** BinaryClassification.ipynb\n",
    "\n",
    "**Course:** Data Science Foundations: Data Mining in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTALL AND IMPORT LIBRARIES\n",
    "\n",
    "The Python library `nltk`, for \"Natural Language Toolkit,\" contains most of the functions we need for text mining. NLTK can be installed with Python's `pip` command. This command only needs to be done once per machine.\n",
    "\n",
    "The standard, shorter approach may work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command didn't work, it may be necessary to be more explicit, in which case you could run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `nltk` is installed, then load the libraries and data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re  # For regular expressions\n",
    "import nltk  # For text functions\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import pandas as pd  # For dataframes\n",
    "\n",
    "# Import corpora and functions from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download data for NLTK\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('opinion_lexicon', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Use Matplotlib style sheet\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Iliad.txt',sep='\\t')\\\n",
    "    .dropna()\\\n",
    "    .drop('gutenberg_id', 1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE DATA\n",
    "\n",
    "\n",
    "## Tokenize the Data\n",
    "\n",
    "- A \"token\" is the level of analysis for text mining.\n",
    "- In this case, the tokens will be individual words, which is most common, but tokens can also be pairs or triplets of words, sentences, and so on.\n",
    "- In the tokenization process, it is common to standardize capitalization and remove non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert all text to lowercase\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = re.sub(r'[^\\w]', ' ', text)  # Leave only word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Omit extra space characters\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].map(clean_text) \n",
    "df['text'] = df['text'].map(word_tokenize) # Split text into word tokens\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Tokens into a Single Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.text.explode().to_frame('token')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Tokens by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words\n",
    "\n",
    "- Stop words are common words such as \"the,\" \"and\", and \"a\" that may interfere with the sementic analysis of text.\n",
    "- It is common to use a lexicon or established list of stop words.\n",
    "- However, different stop word lexiconss may process text differently.\n",
    "- It is also possible to add specific words to a custom stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english')) # load stopwords\n",
    "\n",
    "df = df[~df.token.isin(stopwords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Revised Tokens by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFY SENTIMENTS\n",
    "\n",
    "## Identify Valenced Words with the \"Opinion\" Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon = {\n",
    "    **{w: 'positive' for w in opinion_lexicon.positive()},\n",
    "    **{w: 'negative' for w in opinion_lexicon.negative()}\n",
    "}\n",
    "\n",
    "df['sentiment'] = df['token'].map(sentiment_lexicon)\n",
    "df = df[~df.sentiment.isna()] # ommit words out of opinion lexicon\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Sentiment Words by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the Sentiment Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = df.sentiment.value_counts().to_frame('n')\n",
    "summary_df['prop'] = summary_df['n'] / summary_df.n.sum()\n",
    "\n",
    "summary_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.n.plot.bar(legend=False, figsize=(8, 4), grid=True, color='gray')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency of Words')\n",
    "plt.title('The Iliad: Proportion of positive and negative words', loc='left')\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN UP\n",
    "\n",
    "- If desired, clear the results with Cell > All Output > Clear. \n",
    "- Save your work by selecting File > Save and Checkpoint.\n",
    "- Shut down the Python kernel and close the file by selecting File > Close and Halt."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
