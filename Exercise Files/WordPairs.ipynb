{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD PAIRS\n",
    "\n",
    "**File:** WordPairs.ipynb\n",
    "\n",
    "**Course:** Data Science Foundations: Data Mining in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTALL AND IMPORT LIBRARIES\n",
    "\n",
    "To explore the connections between words, we'll use the Python library `networkx`. It can be installed with Python's `pip` command. This command only needs to be done once per machine.\n",
    "\n",
    "The standard, shorter approach may work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command didn't work, it may be necessary to be more explicit, in which case you could run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `networkx` is installed, then load the libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re  # For regular expressions\n",
    "import nltk  # For text functions\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import pandas as pd  # For dataframes\n",
    "import networkx as nx  # For network graphs\n",
    "\n",
    "# Import specific text functions from NLTK\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download data for NLTK\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('opinion_lexicon', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Iliad.txt',sep='\\t')\\\n",
    "    .dropna() \\\n",
    "    .drop('gutenberg_id', 1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE DATA\n",
    "\n",
    "\n",
    "## Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() # lowecase\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = re.sub(r'[^\\w]', ' ', text) # leave only word characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # ommit extra space characters\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "df['text'] = df['text'].map(clean_text) \n",
    "df['text'] = df['text'].map(word_tokenize) # Split text into words\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE WORD PAIR TOKENS\n",
    "\n",
    "- Instead of splitting the text into single words, separate it into pairs of adjacent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wordpairs'] = df['text'].map(lambda x: list(ngrams(x, 2)))\n",
    "df = df.explode('wordpairs')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Tokens by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wordpairs'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Word Pairs\n",
    "\n",
    "- In order to remove word pairs with stop words, the pairs must first be separated.\n",
    "- Separated pairs are also necessary for creating network graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df.wordpairs.values.tolist(), columns=['word1', 'word2']).dropna()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the number of rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words\n",
    "\n",
    "- This reduces the total number of observations from 127,709 to 33,694, a 74% reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "df = df[~(df.word1.isin(en_stopwords) | df.word2.isin(en_stopwords))]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the new number of rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Word Pairs by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['word1', 'word2'])\\\n",
    "    .size()\\\n",
    "    .to_frame('n')\\\n",
    "    .reset_index()\\\n",
    "    .sort_values('n', ascending=False)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE DATA\n",
    "\n",
    "## Create Data Table\n",
    "\n",
    "- Restrict to word pairs that appear more than 12 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.n > 12].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Network Graph\n",
    "\n",
    "- For clarity's sake, restrict to word pairs that appear more than 25 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df[df.n > 25], 'word1', 'word2')\n",
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw_shell(G, with_labels=True, node_color='white', font_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN UP\n",
    "\n",
    "- If desired, clear the results with Cell > All Output > Clear. \n",
    "- Save your work by selecting File > Save and Checkpoint.\n",
    "- Shut down the Python kernel and close the file by selecting File > Close and Halt."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
